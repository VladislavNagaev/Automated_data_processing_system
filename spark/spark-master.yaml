version: '3.8'

services:

  spark-master:
    # image: bde2020/spark-master:3.2.0-hadoop3.2
    build: .
    container_name: spark-master
    hostname: spark-master
    networks: 
      - main-overlay-network
    depends_on:
      - hadoop-namenode
      - hadoop-datanode-1
      - hadoop-datanode-2
    ports:
      - 8090:8080
      - 7077:7077
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    healthcheck:
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  spark-worker-1:
    image: bde2020/spark-worker:3.2.0-hadoop3.2
    container_name: spark-worker-1
    networks: 
      - main-overlay-network
    depends_on:
      - spark-master
    ports:
      - 8091:8081
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    healthcheck:
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  spark-history-server:
    image: bde2020/spark-history-server:3.2.0-hadoop3.2
    container_name: spark-history-server
    networks: 
      - main-overlay-network
    restart: always
    depends_on:
      - spark-master
    ports:
      - 18081:18081
    volumes:
      - type: bind
        source: ./spark-data/spark-history-server
        target: /tmp/spark-events

networks:
  main-overlay-network:
    external: true
    driver: overlay
    attachable: true
